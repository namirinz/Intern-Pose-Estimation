{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simple-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "signal-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nath_intern/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      5280  models.common.Focus                     [3, 48, 3]                    \n",
      "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
      "  2                -1  1     65280  models.common.C3                        [96, 96, 2]                   \n",
      "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
      "  4                -1  1    629760  models.common.C3                        [192, 192, 6]                 \n",
      "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
      "  6                -1  1   2512896  models.common.C3                        [384, 384, 6]                 \n",
      "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
      "  8                -1  1   1476864  models.common.SPP                       [768, 768, [5, 9, 13]]        \n",
      "  9                -1  1   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1   1182720  models.common.C3                        [768, 384, 2, False]          \n",
      " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1    296448  models.common.C3                        [384, 192, 2, False]          \n",
      " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1   1035264  models.common.C3                        [384, 384, 2, False]          \n",
      " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 24      [17, 20, 23]  1    343485  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
      "Model Summary: 391 layers, 21375645 parameters, 21375645 gradients, 51.4 GFLOPS\n",
      "\n",
      "YOLOv5 ðŸš€ 2021-3-31 torch 1.7.1 CUDA:0 (GeForce RTX 2070 SUPER, 7982.3125MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5m')\n",
    "\n",
    "# S -> tiki-taka : 68 fps\n",
    "# M -> tiki-taka : 60 fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sudden-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.device('cpu'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "surrounded-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.half();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "still-string",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "animated-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.classes = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "automatic-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "video_path = 'videos/kbank-cctv-1.mp4'\n",
    "out_video_path = 'visual_result/kbank-cctv-2.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "count = 0\n",
    "n_frame_skip = 1\n",
    "\n",
    "videoWriter = cv2.VideoWriter(out_video_path, fourcc, fps, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "norwegian-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30.06376126226387, Size: (1376, 776)\n"
     ]
    }
   ],
   "source": [
    "print(f'FPS: {fps}, Size: {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amazing-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT_SCALE = 1\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_COLOR = (39, 0, 247)\n",
    "FONT_THICKNESS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    flag, img = cap.read()\n",
    "    if not flag:\n",
    "        break\n",
    "    s1 = time.time()\n",
    "    \n",
    "    results = model(img, size=400)\n",
    "    results.imgs\n",
    "    results.render()\n",
    "    vis_img = results.imgs[0]\n",
    "    \n",
    "    \n",
    "    fps_process = 1 / (time.time()-s1)\n",
    "    \n",
    "    cv2.putText(vis_img,f'FPS {fps_process:.2f}', (size[0]-200, 30),  FONT_FACE, FONT_SCALE, FONT_COLOR, FONT_THICKNESS)    \n",
    "    cv2.imshow('Image', vis_img)\n",
    "    \n",
    "    #if save_out_video:\n",
    "    videoWriter.write(vis_img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    # Frame Skipping\n",
    "    #count += n_frame_skip\n",
    "    #cap.set(1, count)\n",
    "\n",
    "cap.release()\n",
    "videoWriter.release()\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5_env",
   "language": "python",
   "name": "yolov5_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
